{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iCGERDM6UKd_"
   },
   "source": [
    "# **Topic Model Evaluation**\n",
    "Here, you will find the code needed to run the experiments of the paper:\n",
    "\n",
    "*BERTopic: Neural topic modeling with a class-based TF-IDF procedure*.\n",
    "\n",
    "The package itself can be found [here](https://github.com/MaartenGr/BERTopic) and the repository for evaluation [here]()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MFwg78cJ0BFg"
   },
   "source": [
    "## **Installation**\n",
    "First, we need to install a few packages in order to run our experiments. Most of the packages are installed through the `tm_evaluation` package of which [OCTIS](https://github.com/MIND-Lab/OCTIS) is an important component. \n",
    "\n",
    "You can install the evaluation package with `pip install .` from the root. To additionally install CTM run `pip install .[ctm]`To install BERTopic, run `pip install bertopic==v0.9.4` after installing the base package or use `pip install .[bertopic]`. Top2Vec should be installed with `pip install top2vec==v1.0.26` after installing the base package. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dLesRC-17zN9"
   },
   "source": [
    "To run a faster version of LDAseq for dynamic topic modeling, we need to uninstall gensim and install a specific merge that allows for this speed-up. First, run `pip uninstall gensim -y`, then, run `pip install git+https://github.com/RaRe-Technologies/gensim.git@refs/pull/3172/merge`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fisNVVa977e-"
   },
   "source": [
    "**NOTE**: After installing the above packages, make sure to restart the runtime otherwise you are likely to run into issues. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nQnAdKJ-Do1S"
   },
   "source": [
    "#  1. **Data**\n",
    "Some of the data can be accessed through OCTIS, such as the `20NewsGroup` and `BBC_News` datasets. Other datasets, however, are downloaded and then run through OCTIS in order to be used in their pipeline. \n",
    "\n",
    "The datasets that we are going to be preparing are: \n",
    "* Trump's tweets\n",
    "* United Nations general debates between 2006 and 2015 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fionaguo/.pyenv/versions/3.10.4/envs/topic_model_env/lib/python3.10/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/Users/fionaguo/.pyenv/versions/3.10.4/envs/topic_model_env/lib/python3.10/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/Users/fionaguo/.pyenv/versions/3.10.4/envs/topic_model_env/lib/python3.10/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/Users/fionaguo/.pyenv/versions/3.10.4/envs/topic_model_env/lib/python3.10/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "[nltk_data] Downloading package punkt to /Users/fionaguo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import bertopic\n",
    "import octis\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src/BERTopic_evaluation')\n",
    "import evaluation\n",
    "from evaluation import Trainer, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_screen_name</th>\n",
       "      <th>user_verified</th>\n",
       "      <th>user_followers_count</th>\n",
       "      <th>user_friends_count</th>\n",
       "      <th>user_favourites_count</th>\n",
       "      <th>processed</th>\n",
       "      <th>...</th>\n",
       "      <th>fairness</th>\n",
       "      <th>cheating</th>\n",
       "      <th>loyalty</th>\n",
       "      <th>betrayal</th>\n",
       "      <th>authority</th>\n",
       "      <th>subversion</th>\n",
       "      <th>purity</th>\n",
       "      <th>degradation</th>\n",
       "      <th>topic_idx</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.268960e+18</td>\n",
       "      <td>2020-06-05 17:36:34+00:00</td>\n",
       "      <td>thank you for continuing to discover and suppo...</td>\n",
       "      <td>1043248881817989120</td>\n",
       "      <td>EDGEOUTRecords</td>\n",
       "      <td>False</td>\n",
       "      <td>33</td>\n",
       "      <td>27</td>\n",
       "      <td>41</td>\n",
       "      <td>thank you user for continuing to discover and ...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1_covid_her_she_music</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.268955e+18</td>\n",
       "      <td>2020-06-05 17:16:02+00:00</td>\n",
       "      <td>successfully society matters</td>\n",
       "      <td>1258900480291266560</td>\n",
       "      <td>JuleneHart8</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>user successfully society matters</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4075</td>\n",
       "      <td>4075_society_norms_phinna_wackadoodle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.269131e+18</td>\n",
       "      <td>2020-06-06 04:56:26+00:00</td>\n",
       "      <td>why does every single boy i meet up with end u...</td>\n",
       "      <td>3098710962</td>\n",
       "      <td>Yess_Duhh</td>\n",
       "      <td>False</td>\n",
       "      <td>331</td>\n",
       "      <td>820</td>\n",
       "      <td>6689</td>\n",
       "      <td>why does every single boy i meet up with end u...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6321</td>\n",
       "      <td>6321_boys_wwho_triflin_fuckboy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.269009e+18</td>\n",
       "      <td>2020-06-05 20:52:49+00:00</td>\n",
       "      <td>this shirt got me and my melanin fucked up</td>\n",
       "      <td>219421984</td>\n",
       "      <td>xolovedanielle</td>\n",
       "      <td>False</td>\n",
       "      <td>1130</td>\n",
       "      <td>375</td>\n",
       "      <td>66873</td>\n",
       "      <td>hahahahahahahaha this shirt got me and my mela...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1_covid_her_she_music</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.269154e+18</td>\n",
       "      <td>2020-06-06 06:27:55+00:00</td>\n",
       "      <td>department of justice investigate the killing ...</td>\n",
       "      <td>2217137054</td>\n",
       "      <td>christtinal</td>\n",
       "      <td>False</td>\n",
       "      <td>567</td>\n",
       "      <td>263</td>\n",
       "      <td>49775</td>\n",
       "      <td>department of justice investigate the killing ...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7994</td>\n",
       "      <td>7994_tamir_appoint_investigate_prosecutor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1136209</th>\n",
       "      <td>1.264263e+18</td>\n",
       "      <td>2020-05-23 18:32:01+00:00</td>\n",
       "      <td>depression sucks ass that s the tweet</td>\n",
       "      <td>1083572736625000448</td>\n",
       "      <td>Jei_757</td>\n",
       "      <td>False</td>\n",
       "      <td>243</td>\n",
       "      <td>311</td>\n",
       "      <td>11618</td>\n",
       "      <td>depression sucks ass thats the tweet cantgetou...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1_covid_her_she_music</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1136210</th>\n",
       "      <td>1.264286e+18</td>\n",
       "      <td>2020-05-23 20:02:37+00:00</td>\n",
       "      <td>toilets were definitely designed by a woman</td>\n",
       "      <td>38345895</td>\n",
       "      <td>OddSoull_</td>\n",
       "      <td>False</td>\n",
       "      <td>308</td>\n",
       "      <td>255</td>\n",
       "      <td>445</td>\n",
       "      <td>toilets were definitely designed by a woman</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1154</td>\n",
       "      <td>1154_restroom_bathroom_restrooms_bathrooms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1136211</th>\n",
       "      <td>1.264273e+18</td>\n",
       "      <td>2020-05-23 19:12:57+00:00</td>\n",
       "      <td>imagine seeing bron biking in la that s lit</td>\n",
       "      <td>1358850445</td>\n",
       "      <td>T_RAWTheGreat</td>\n",
       "      <td>False</td>\n",
       "      <td>166</td>\n",
       "      <td>98</td>\n",
       "      <td>10831</td>\n",
       "      <td>imagine seeing 68 bron biking in la grinningfa...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1_covid_her_she_music</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1136212</th>\n",
       "      <td>1.264225e+18</td>\n",
       "      <td>2020-05-23 16:03:37+00:00</td>\n",
       "      <td>the lockdown has done untold damage to this co...</td>\n",
       "      <td>59047876</td>\n",
       "      <td>sophiatseliem</td>\n",
       "      <td>False</td>\n",
       "      <td>170</td>\n",
       "      <td>249</td>\n",
       "      <td>2502</td>\n",
       "      <td>the lockdown has done untold damage to this co...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8772</td>\n",
       "      <td>8772_lockdown_choitold_wellclasses_lockdowns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1136213</th>\n",
       "      <td>1.264450e+18</td>\n",
       "      <td>2020-05-24 06:53:50+00:00</td>\n",
       "      <td>love is the crazy drug</td>\n",
       "      <td>1139990092477370368</td>\n",
       "      <td>Danny75560506</td>\n",
       "      <td>False</td>\n",
       "      <td>1379</td>\n",
       "      <td>1442</td>\n",
       "      <td>45892</td>\n",
       "      <td>user user user user user user user user love i...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>368</td>\n",
       "      <td>368_drugs_cocaine_drug_heroin</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1136214 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id                 created_at  \\\n",
       "0        1.268960e+18  2020-06-05 17:36:34+00:00   \n",
       "1        1.268955e+18  2020-06-05 17:16:02+00:00   \n",
       "2        1.269131e+18  2020-06-06 04:56:26+00:00   \n",
       "3        1.269009e+18  2020-06-05 20:52:49+00:00   \n",
       "4        1.269154e+18  2020-06-06 06:27:55+00:00   \n",
       "...               ...                        ...   \n",
       "1136209  1.264263e+18  2020-05-23 18:32:01+00:00   \n",
       "1136210  1.264286e+18  2020-05-23 20:02:37+00:00   \n",
       "1136211  1.264273e+18  2020-05-23 19:12:57+00:00   \n",
       "1136212  1.264225e+18  2020-05-23 16:03:37+00:00   \n",
       "1136213  1.264450e+18  2020-05-24 06:53:50+00:00   \n",
       "\n",
       "                                                      text  \\\n",
       "0        thank you for continuing to discover and suppo...   \n",
       "1                             successfully society matters   \n",
       "2        why does every single boy i meet up with end u...   \n",
       "3               this shirt got me and my melanin fucked up   \n",
       "4        department of justice investigate the killing ...   \n",
       "...                                                    ...   \n",
       "1136209              depression sucks ass that s the tweet   \n",
       "1136210        toilets were definitely designed by a woman   \n",
       "1136211        imagine seeing bron biking in la that s lit   \n",
       "1136212  the lockdown has done untold damage to this co...   \n",
       "1136213                             love is the crazy drug   \n",
       "\n",
       "                     user_id user_screen_name  user_verified  \\\n",
       "0        1043248881817989120   EDGEOUTRecords          False   \n",
       "1        1258900480291266560      JuleneHart8          False   \n",
       "2                 3098710962        Yess_Duhh          False   \n",
       "3                  219421984   xolovedanielle          False   \n",
       "4                 2217137054      christtinal          False   \n",
       "...                      ...              ...            ...   \n",
       "1136209  1083572736625000448          Jei_757          False   \n",
       "1136210             38345895        OddSoull_          False   \n",
       "1136211           1358850445    T_RAWTheGreat          False   \n",
       "1136212             59047876    sophiatseliem          False   \n",
       "1136213  1139990092477370368    Danny75560506          False   \n",
       "\n",
       "         user_followers_count  user_friends_count  user_favourites_count  \\\n",
       "0                          33                  27                     41   \n",
       "1                           1                  50                      2   \n",
       "2                         331                 820                   6689   \n",
       "3                        1130                 375                  66873   \n",
       "4                         567                 263                  49775   \n",
       "...                       ...                 ...                    ...   \n",
       "1136209                   243                 311                  11618   \n",
       "1136210                   308                 255                    445   \n",
       "1136211                   166                  98                  10831   \n",
       "1136212                   170                 249                   2502   \n",
       "1136213                  1379                1442                  45892   \n",
       "\n",
       "                                                 processed  ...  fairness  \\\n",
       "0        thank you user for continuing to discover and ...  ...       0.0   \n",
       "1                        user successfully society matters  ...       0.0   \n",
       "2        why does every single boy i meet up with end u...  ...       0.0   \n",
       "3        hahahahahahahaha this shirt got me and my mela...  ...       0.0   \n",
       "4        department of justice investigate the killing ...  ...       0.0   \n",
       "...                                                    ...  ...       ...   \n",
       "1136209  depression sucks ass thats the tweet cantgetou...  ...       0.0   \n",
       "1136210        toilets were definitely designed by a woman  ...       0.0   \n",
       "1136211  imagine seeing 68 bron biking in la grinningfa...  ...       0.0   \n",
       "1136212  the lockdown has done untold damage to this co...  ...       0.0   \n",
       "1136213  user user user user user user user user love i...  ...       0.0   \n",
       "\n",
       "         cheating  loyalty  betrayal  authority  subversion  purity  \\\n",
       "0             0.0      0.0       0.0        0.0         0.0     0.0   \n",
       "1             0.0      1.0       0.0        0.0         0.0     0.0   \n",
       "2             0.0      0.0       0.0        0.0         0.0     0.0   \n",
       "3             0.0      0.0       0.0        0.0         0.0     0.0   \n",
       "4             0.0      0.0       0.0        0.0         0.0     0.0   \n",
       "...           ...      ...       ...        ...         ...     ...   \n",
       "1136209       0.0      0.0       0.0        0.0         0.0     0.0   \n",
       "1136210       0.0      0.0       0.0        0.0         0.0     0.0   \n",
       "1136211       0.0      0.0       0.0        0.0         0.0     0.0   \n",
       "1136212       0.0      0.0       0.0        0.0         0.0     0.0   \n",
       "1136213       0.0      0.0       0.0        0.0         0.0     0.0   \n",
       "\n",
       "         degradation  topic_idx                                         topic  \n",
       "0                0.0         -1                        -1_covid_her_she_music  \n",
       "1                0.0       4075         4075_society_norms_phinna_wackadoodle  \n",
       "2                0.0       6321                6321_boys_wwho_triflin_fuckboy  \n",
       "3                0.0         -1                        -1_covid_her_she_music  \n",
       "4                0.0       7994     7994_tamir_appoint_investigate_prosecutor  \n",
       "...              ...        ...                                           ...  \n",
       "1136209          0.0         -1                        -1_covid_her_she_music  \n",
       "1136210          0.0       1154    1154_restroom_bathroom_restrooms_bathrooms  \n",
       "1136211          0.0         -1                        -1_covid_her_she_music  \n",
       "1136212          0.0       8772  8772_lockdown_choitold_wellclasses_lockdowns  \n",
       "1136213          0.0        368                 368_drugs_cocaine_drug_heroin  \n",
       "\n",
       "[1136214 rows x 33 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../LA_data/LA_tweets_10perc_sample_emot_mf_before20200801_topics.csv',lineterminator='\\n')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created vocab\n",
      "259009\n",
      "words filtering done\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(dataset=\"LA\")\n",
    "dataloader.prepare_docs(save=\"LA.txt\", docs=df.text.tolist()).preprocess_octis(output_folder=\"LA\")\n",
    "dataloader.timestamps = df.created_at.apply(lambda x: x[:-6]).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YLvF2devjGZt"
   },
   "source": [
    "## Trump\n",
    "The data can be found here: https://www.thetrumparchive.com/faq\n",
    "\n",
    "Using our `DataLoader` we can prepare the documents and save them in an OCTIS-based format: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N-X1poWssOoQ",
    "outputId": "5c6b14ff-0ce3-43f0-d18e-fa3952cba31b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%%time` not found.\n"
     ]
    }
   ],
   "source": [
    "# creates a trump.txt file in which each line is a document\n",
    "# creates a folder with vocab and metadata etc\n",
    "%%time\n",
    "dataloader = DataLoader(dataset=\"trump\").prepare_docs(save=\"trump.txt\").preprocess_octis(output_folder=\"trump\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wV-GDOko9CaC"
   },
   "source": [
    "Additionally, there isa DTM variant that creates 10 timesteps to be used in the dynamic topic modeling experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i9P7qS9Qp56K"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "dataloader = DataLoader(dataset=\"trump_dtm\").prepare_docs(save=\"trump_dtm.txt\").preprocess_octis(output_folder=\"trump_dtm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GRJJvgHyjYa6"
   },
   "source": [
    "# 2. **Evaluation**\n",
    "After preparing our data, we can start evaluating the topic models as used in the experiments. OCTIS already has a number of models prepared that we can use directly as shown below. \n",
    "\n",
    "First, we specify what the dataset is and whether that was a custom dataset not found in OCTIS. To run our custom trump dataset, we run `dataset, custom = \"trump\", True`. In contrast, if we are to use the prepackaged 20NewsGroup dataset, we run `dataset, custom = \"20NewsGroup\", False` instead. \n",
    "\n",
    "The OCTIS datasets can be found [here](https://github.com/MIND-Lab/OCTIS#available-datasets). \n",
    "\n",
    "Second, we define a number of parameters to be used for the model. It uses the following format: \n",
    "\n",
    "`params = {\"num_topics\": [(i+1)*10 for i in range(5)], \"random_state\": random_state}`\n",
    "\n",
    "were we define a number of topics to loop over and calculate the evluation metrics but also define a number of parameters used in the models. \n",
    "\n",
    "#### **Parameters**\n",
    "The parameters for LDA and NMF:\n",
    "\n",
    "\n",
    "```python\n",
    "params = {\"num_topics\": [(i+1)*10 for i in range(5)], \"random_state\": random_state}`\n",
    "```\n",
    "\n",
    "The parameters for Top2Vec:\n",
    "\n",
    "```python\n",
    "params = {\"nr_topics\": [(i+1)*10 for i in range(5)],\n",
    "          \"hdbscan_args\": {'min_cluster_size': 15,\n",
    "                            'metric': 'euclidean',\n",
    "                            'cluster_selection_method': 'eom'}}\n",
    "```\n",
    "Note that the `min_cluster_size` is 15 for all datasets except BBC_News.\n",
    "\n",
    "The parameters for CTM:\n",
    "\n",
    "```python\n",
    "params = {\n",
    "    \"n_components\": [(i+1)*10 for i in range(5)],\n",
    "    \"contextual_size\":768\n",
    "}\n",
    "```\n",
    "\n",
    "The parameters for BERTopic:\n",
    "\n",
    "```python\n",
    "params = {\n",
    "    \"nr_topics\": [(i+1)*10 for i in range(5)],\n",
    "    \"min_topic_size\": 15,\n",
    "    \"verbose\": True\n",
    "}\n",
    "```\n",
    "\n",
    "Note that the `min_topic_size` is 15 for all datasets except BBC_News. Note that we do not set a `embedding_model` here. We do this on purpose as we can generate the embeddings beforehand and pass those to BERTopic. \n",
    "\n",
    "\n",
    "**Evaluation Metrics**\n",
    "\n",
    "For each topic model, its topic coherence was evaluated using normalized pointwise mutual information (NPMI,\n",
    "(Bouma, 2009)). This coherence measure has been shown to emulate human judgment with reasonable performance (Lau et al., 2014). The measure ranges from [-1, 1] where 1 indicates a perfect association. \n",
    "\n",
    "Topic diversity, as defined by (Dieng et al., 2020), is the percentage of unique words for all topics. The measure ranges from [0, 1] where 0 indicates redundant topics and 1 indicates more varied topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZOhlHj4SGush"
   },
   "source": [
    "## **OCTIS**\n",
    "Here, we can run the experiments for NMF and LDA. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hu1kVxBhA4iY"
   },
   "source": [
    "#### NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AOZl2Xoed_3a"
   },
   "outputs": [],
   "source": [
    "for i, random_state in enumerate([0, 21, 42]):\n",
    "    dataset, custom = \"trump\", True\n",
    "    params = {\"num_topics\": [(i+1)*10 for i in range(5)], \"random_state\": random_state}\n",
    "\n",
    "    trainer = Trainer(dataset=dataset,\n",
    "                      model_name=\"NMF\",\n",
    "                      params=params,\n",
    "                      custom_dataset=custom,\n",
    "                      verbose=True)\n",
    "    results = trainer.train(save=f\"NMF_trump_{i+1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l8s9zjC5A6JD"
   },
   "source": [
    "#### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "uAZ_ckYBeFDY",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.052112391394907386\n",
      "diversity: 0.7\n",
      " \n",
      "Results\n",
      "============\n",
      "npmi: 0.04179319292310928\n",
      "diversity: 0.765\n",
      " \n",
      "Results\n",
      "============\n",
      "npmi: 0.03377454595193811\n",
      "diversity: 0.76\n",
      " \n",
      "Results\n",
      "============\n",
      "npmi: 0.028185342061211476\n",
      "diversity: 0.8175\n",
      " \n",
      "Results\n",
      "============\n",
      "npmi: 0.014312795758256154\n",
      "diversity: 0.758\n",
      " \n",
      "Results\n",
      "============\n",
      "npmi: 0.06510597194306655\n",
      "diversity: 0.71\n",
      " \n",
      "Results\n",
      "============\n",
      "npmi: 0.0385227423973146\n",
      "diversity: 0.72\n",
      " \n",
      "Results\n",
      "============\n",
      "npmi: 0.03203997656642418\n",
      "diversity: 0.7633333333333333\n",
      " \n",
      "Results\n",
      "============\n",
      "npmi: 0.01896258049952331\n",
      "diversity: 0.795\n",
      " \n",
      "Results\n",
      "============\n",
      "npmi: 0.020985759593940206\n",
      "diversity: 0.756\n",
      " \n",
      "Results\n",
      "============\n",
      "npmi: 0.04593154813272515\n",
      "diversity: 0.78\n",
      " \n",
      "Results\n",
      "============\n",
      "npmi: 0.027467765401634973\n",
      "diversity: 0.75\n",
      " \n",
      "Results\n",
      "============\n",
      "npmi: 0.029733949088544435\n",
      "diversity: 0.7733333333333333\n",
      " \n",
      "Results\n",
      "============\n",
      "npmi: 0.023507847047261828\n",
      "diversity: 0.8\n",
      " \n",
      "Results\n",
      "============\n",
      "npmi: 0.015394291218036302\n",
      "diversity: 0.826\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for i, random_state in enumerate([0, 21, 42]):\n",
    "    dataset, custom = \"LA\", True\n",
    "    params = {\"num_topics\": [(i+1)*10 for i in range(5)], \"random_state\": random_state}\n",
    "\n",
    "    trainer = Trainer(dataset=dataset,\n",
    "                      model_name=\"LDA\",\n",
    "                      params=params,\n",
    "                      custom_dataset=custom,\n",
    "                      verbose=True)\n",
    "    results_lda = trainer.train(save=f\"LDA_LAdata_{i+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "results_lda = []\n",
    "for i in [1,2,3]:\n",
    "    with open(f'LDA_LAdata_{i}.json','rb') as f:\n",
    "        results_lda.extend(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NPMI: mean = 0.032522046665192934, std = 0.013721846540217773\n",
      "diversity: mean = 0.7649444444444444, std = 0.03517046670709763\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "npmi_lda = []\n",
    "diversity_lda = []\n",
    "for i in results_lda:\n",
    "    npmi_lda.append(i['Scores']['npmi'])\n",
    "    diversity_lda.append(i['Scores']['diversity'])\n",
    "    \n",
    "print(f'NPMI: mean = {np.mean(npmi_lda)}, std = {np.std(npmi_lda)}')\n",
    "print(f'diversity: mean = {np.mean(diversity_lda)}, std = {np.std(diversity_lda)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZsRdU70OVsA4"
   },
   "source": [
    "## **CTM**\n",
    "Here, we use de CombinedTM of the Contextualized Topic Models:  https://github.com/MilaNLProc/contextualized-topic-models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zPUkcq9hXsV8"
   },
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    dataset, custom = \"trump\", True\n",
    "    params = {\n",
    "        \"n_components\": [(i+1)*10 for i in range(5)],\n",
    "        \"contextual_size\":768\n",
    "    }\n",
    "\n",
    "    trainer = Trainer(dataset=dataset,\n",
    "                      model_name=\"CTM_CUSTOM\",\n",
    "                      params=params,\n",
    "                      custom_dataset=custom,\n",
    "                      verbose=True)\n",
    "    results = trainer.train(save=f\"CTM_trump_{i+1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CGhfpkBZGtRl"
   },
   "source": [
    "## **BERTopic**\n",
    "\n",
    "To speed up BERTopic, we can generate the embeddings before passing it to the `Trainer`. This way, the same embeddings do not have to be generated 5 times which speeds up evaluation quite a bit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "XhFXGK8oxVpi"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Prepare data\n",
    "dataset, custom = \"LA\", True\n",
    "data_loader = DataLoader(dataset)\n",
    "_, timestamps = data_loader.load_docs()\n",
    "data = data_loader.load_octis(custom)\n",
    "data = [\" \".join(words) for words in data.get_corpus()]\n",
    "\n",
    "# Extract embeddings\n",
    "model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "embeddings = model.encode(data, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('LA_10perc_embeddings.pickle','wb') as f:\n",
    "    pickle.dump(embeddings,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1136214, 768)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nb1afGtJEuw0"
   },
   "source": [
    "As show above, we load in the `data` which the data loader and combine the tokens in each document to generate our training data. Then, we pass it to the sentence transformer model of our choice and generate the embeddings. \n",
    "\n",
    "Next, we pass these embeddings to the `bt_embeddings` parameter to speed up training: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "II0a3WJP37V0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-15 18:35:40,523 - BERTopic - Reduced dimensionality with UMAP\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-15 18:38:14,681 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-06-15 18:44:30,661 - BERTopic - Reduced number of topics from 6202 to 11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.14577935806182138\n",
      "diversity: 0.78\n",
      " \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 16\u001b[0m\n\u001b[1;32m      2\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding_model\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall-mpnet-base-v2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnr_topics\u001b[39m\u001b[38;5;124m\"\u001b[39m: [(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m)],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mverbose\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      8\u001b[0m }\n\u001b[1;32m     10\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(dataset\u001b[38;5;241m=\u001b[39mdataset,\n\u001b[1;32m     11\u001b[0m                   model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBERTopic\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m                   params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[1;32m     13\u001b[0m                   bt_embeddings\u001b[38;5;241m=\u001b[39membeddings,\n\u001b[1;32m     14\u001b[0m                   custom_dataset\u001b[38;5;241m=\u001b[39mcustom,\n\u001b[1;32m     15\u001b[0m                   verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 16\u001b[0m results_bertopic \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBERTopic_LAdata_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/INCAS/event_changes/notebooks/../src/BERTopic_evaluation/evaluation/evaluation.py:164\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, save)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param_combo \u001b[38;5;129;01min\u001b[39;00m new_params:\n\u001b[1;32m    159\u001b[0m \n\u001b[1;32m    160\u001b[0m     \u001b[38;5;66;03m# Train and evaluate model\u001b[39;00m\n\u001b[1;32m    161\u001b[0m     params_to_use \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    162\u001b[0m         param: value \u001b[38;5;28;01mfor\u001b[39;00m param, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(params_name, param_combo)\n\u001b[1;32m    163\u001b[0m     }\n\u001b[0;32m--> 164\u001b[0m     output, computation_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_tm_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_to_use\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m     scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate(output)\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# Update results\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/INCAS/event_changes/notebooks/../src/BERTopic_evaluation/evaluation/evaluation.py:203\u001b[0m, in \u001b[0;36mTrainer._train_tm_model\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# Train BERTopic\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBERTopic\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_bertopic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;66;03m# Train Top2Vec\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTop2Vec\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/INCAS/event_changes/notebooks/../src/BERTopic_evaluation/evaluation/evaluation.py:409\u001b[0m, in \u001b[0;36mTrainer._train_bertopic\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    406\u001b[0m     model \u001b[38;5;241m=\u001b[39m BERTopic(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m    408\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 409\u001b[0m topics, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;66;03m# Dynamic Topic Modeling\u001b[39;00m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimestamps:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/envs/topic_model_env/lib/python3.10/site-packages/bertopic/_bertopic.py:298\u001b[0m, in \u001b[0;36mBERTopic.fit_transform\u001b[0;34m(self, documents, embeddings, y)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseed_topic_list \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    297\u001b[0m     y, embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_guided_topic_modeling(embeddings)\n\u001b[0;32m--> 298\u001b[0m umap_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reduce_dimensionality\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;66;03m# Cluster UMAP embeddings with HDBSCAN\u001b[39;00m\n\u001b[1;32m    301\u001b[0m documents, probabilities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cluster_embeddings(umap_embeddings, documents)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/envs/topic_model_env/lib/python3.10/site-packages/bertopic/_bertopic.py:1377\u001b[0m, in \u001b[0;36mBERTopic._reduce_dimensionality\u001b[0;34m(self, embeddings, y)\u001b[0m\n\u001b[1;32m   1372\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mumap_model \u001b[38;5;241m=\u001b[39m UMAP(n_neighbors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m,\n\u001b[1;32m   1373\u001b[0m                            n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m   1374\u001b[0m                            metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhellinger\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1375\u001b[0m                            low_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory)\u001b[38;5;241m.\u001b[39mfit(embeddings, y\u001b[38;5;241m=\u001b[39my)\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1377\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mumap_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1378\u001b[0m umap_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mumap_model\u001b[38;5;241m.\u001b[39mtransform(embeddings)\n\u001b[1;32m   1379\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReduced dimensionality with UMAP\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/envs/topic_model_env/lib/python3.10/site-packages/umap/umap_.py:2684\u001b[0m, in \u001b[0;36mUMAP.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   2681\u001b[0m     \u001b[38;5;28mprint\u001b[39m(ts(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConstruct embedding\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 2684\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_, aux_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_embed_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2685\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2686\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2687\u001b[0m \u001b[43m        \u001b[49m\u001b[43minit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2688\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# JH why raw data?\u001b[39;49;00m\n\u001b[1;32m   2689\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2690\u001b[0m     \u001b[38;5;66;03m# Assign any points that are fully disconnected from our manifold(s) to have embedding\u001b[39;00m\n\u001b[1;32m   2691\u001b[0m     \u001b[38;5;66;03m# coordinates of np.nan.  These will be filtered by our plotting functions automatically.\u001b[39;00m\n\u001b[1;32m   2692\u001b[0m     \u001b[38;5;66;03m# They also prevent users from being deceived a distance query to one of these points.\u001b[39;00m\n\u001b[1;32m   2693\u001b[0m     \u001b[38;5;66;03m# Might be worth moving this into simplicial_set_embedding or _fit_embed_data\u001b[39;00m\n\u001b[1;32m   2694\u001b[0m     disconnected_vertices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph_\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mflatten() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/envs/topic_model_env/lib/python3.10/site-packages/umap/umap_.py:2717\u001b[0m, in \u001b[0;36mUMAP._fit_embed_data\u001b[0;34m(self, X, n_epochs, init, random_state)\u001b[0m\n\u001b[1;32m   2713\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit_embed_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, n_epochs, init, random_state):\n\u001b[1;32m   2714\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"A method wrapper for simplicial_set_embedding that can be\u001b[39;00m\n\u001b[1;32m   2715\u001b[0m \u001b[38;5;124;03m    replaced by subclasses.\u001b[39;00m\n\u001b[1;32m   2716\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2717\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msimplicial_set_embedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2718\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2719\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2720\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_components\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2721\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initial_alpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2722\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_a\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2723\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_b\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2724\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepulsion_strength\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2725\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnegative_sample_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2726\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2727\u001b[0m \u001b[43m        \u001b[49m\u001b[43minit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2728\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2729\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_distance_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2730\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_metric_kwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2731\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdensmap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2732\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_densmap_kwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2733\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_dens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2734\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_output_distance_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2735\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_output_metric_kwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2736\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_metric\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meuclidean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ml2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2737\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2738\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2739\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtqdm_kwds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtqdm_kwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2740\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/envs/topic_model_env/lib/python3.10/site-packages/umap/umap_.py:1078\u001b[0m, in \u001b[0;36msimplicial_set_embedding\u001b[0;34m(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, densmap, densmap_kwds, output_dens, output_metric, output_metric_kwds, euclidean_output, parallel, verbose, tqdm_kwds)\u001b[0m\n\u001b[1;32m   1073\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m random_state\u001b[38;5;241m.\u001b[39muniform(\n\u001b[1;32m   1074\u001b[0m         low\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m10.0\u001b[39m, high\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10.0\u001b[39m, size\u001b[38;5;241m=\u001b[39m(graph\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], n_components)\n\u001b[1;32m   1075\u001b[0m     )\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m   1076\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(init, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m init \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspectral\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1077\u001b[0m     \u001b[38;5;66;03m# We add a little noise to avoid local minima for optimization to come\u001b[39;00m\n\u001b[0;32m-> 1078\u001b[0m     initialisation \u001b[38;5;241m=\u001b[39m \u001b[43mspectral_layout\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1081\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_components\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric_kwds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_kwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1085\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1086\u001b[0m     expansion \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10.0\u001b[39m \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mabs(initialisation)\u001b[38;5;241m.\u001b[39mmax()\n\u001b[1;32m   1087\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m (initialisation \u001b[38;5;241m*\u001b[39m expansion)\u001b[38;5;241m.\u001b[39mastype(\n\u001b[1;32m   1088\u001b[0m         np\u001b[38;5;241m.\u001b[39mfloat32\n\u001b[1;32m   1089\u001b[0m     ) \u001b[38;5;241m+\u001b[39m random_state\u001b[38;5;241m.\u001b[39mnormal(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1092\u001b[0m         np\u001b[38;5;241m.\u001b[39mfloat32\n\u001b[1;32m   1093\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/envs/topic_model_env/lib/python3.10/site-packages/umap/spectral.py:306\u001b[0m, in \u001b[0;36mspectral_layout\u001b[0;34m(data, graph, dim, random_state, metric, metric_kwds)\u001b[0m\n\u001b[1;32m    303\u001b[0m n_components, labels \u001b[38;5;241m=\u001b[39m scipy\u001b[38;5;241m.\u001b[39msparse\u001b[38;5;241m.\u001b[39mcsgraph\u001b[38;5;241m.\u001b[39mconnected_components(graph)\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_components \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmulti_component_layout\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_components\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric_kwds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_kwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m diag_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(graph\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m    318\u001b[0m \u001b[38;5;66;03m# standard Laplacian\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;66;03m# D = scipy.sparse.spdiags(diag_data, 0, graph.shape[0], graph.shape[0])\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;66;03m# L = D - graph\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;66;03m# Normalized Laplacian\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/envs/topic_model_env/lib/python3.10/site-packages/umap/spectral.py:243\u001b[0m, in \u001b[0;36mmulti_component_layout\u001b[0;34m(data, graph, n_components, component_labels, dim, random_state, metric, metric_kwds)\u001b[0m\n\u001b[1;32m    241\u001b[0m num_lanczos_vectors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m k \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39msqrt(component_graph\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])))\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 243\u001b[0m     eigenvalues, eigenvectors \u001b[38;5;241m=\u001b[39m \u001b[43mscipy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meigsh\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwhich\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSM\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mncv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_lanczos_vectors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mv0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43mL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m     order \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(eigenvalues)[\u001b[38;5;241m1\u001b[39m:k]\n\u001b[1;32m    253\u001b[0m     component_embedding \u001b[38;5;241m=\u001b[39m eigenvectors[:, order]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/envs/topic_model_env/lib/python3.10/site-packages/scipy/sparse/linalg/_eigen/arpack/arpack.py:1697\u001b[0m, in \u001b[0;36meigsh\u001b[0;34m(A, k, M, sigma, which, v0, ncv, maxiter, tol, return_eigenvectors, Minv, OPinv, mode)\u001b[0m\n\u001b[1;32m   1695\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _ARPACK_LOCK:\n\u001b[1;32m   1696\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m params\u001b[38;5;241m.\u001b[39mconverged:\n\u001b[0;32m-> 1697\u001b[0m         \u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1699\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m params\u001b[38;5;241m.\u001b[39mextract(return_eigenvectors)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/envs/topic_model_env/lib/python3.10/site-packages/scipy/sparse/linalg/_eigen/arpack/arpack.py:537\u001b[0m, in \u001b[0;36m_SymmetricArpackParams.iterate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21miterate\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mido, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtol, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresid, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miparam, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mipntr, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m--> 537\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_arpack_solver\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mido\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbmat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhich\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[43m                            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miparam\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m                            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mipntr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mworkd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mworkl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    541\u001b[0m     xslice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mipntr[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mipntr[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn)\n\u001b[1;32m    542\u001b[0m     yslice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mipntr[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mipntr[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    params = {\n",
    "        \"embedding_model\": \"all-mpnet-base-v2\",\n",
    "        \"nr_topics\": [(i+1)*10 for i in range(5)],\n",
    "        \"min_topic_size\": 15,\n",
    "        \"diversity\": None,\n",
    "        \"verbose\": True\n",
    "    }\n",
    "\n",
    "    trainer = Trainer(dataset=dataset,\n",
    "                      model_name=\"BERTopic\",\n",
    "                      params=params,\n",
    "                      bt_embeddings=embeddings,\n",
    "                      custom_dataset=custom,\n",
    "                      verbose=True)\n",
    "    results_bertopic = trainer.train(save=f\"BERTopic_LAdata_{i+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NPMI: mean = 0.13852612603980236, std = 0.01217946063341244\n",
      "diversity: mean = 0.7481777777777779, std = 0.040432223428325406\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "results_BERTopic = []\n",
    "for i in [1,2,3]:\n",
    "    with open(f'BERTopic_LAdata_{i}.json','rb') as f:\n",
    "        results_BERTopic.extend(json.load(f))\n",
    "        \n",
    "import numpy as np\n",
    "\n",
    "npmi_BERTopic = []\n",
    "diversity_BERTopic = []\n",
    "for i in results_BERTopic:\n",
    "    npmi_BERTopic.append(i['Scores']['npmi'])\n",
    "    diversity_BERTopic.append(i['Scores']['diversity'])\n",
    "    \n",
    "print(f'NPMI: mean = {np.mean(npmi_BERTopic)}, std = {np.std(npmi_BERTopic)}')\n",
    "print(f'diversity: mean = {np.mean(diversity_BERTopic)}, std = {np.std(diversity_BERTopic)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xpyBVa1Ka9Hw"
   },
   "source": [
    "## **Top2Vec**\n",
    "Aside from its Doc2Vec backend, we also want to explore its performance using the `\"all-mpnet-base-v2\"` SBERT model as that was used in BERTopic. To do so, we make a very slight change to the core code of Top2Vec, namely replacing all instances of `\"\"distiluse-base-multilingual-cased\"` with `\"all-mpnet-base-v2\"`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pc-PXbV205R3"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import strip_tags\n",
    "import umap\n",
    "import hdbscan\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import dump, load\n",
    "from sklearn.cluster import dbscan\n",
    "import tempfile\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.special import softmax\n",
    "from top2vec import Top2Vec\n",
    "\n",
    "try:\n",
    "    import hnswlib\n",
    "\n",
    "    _HAVE_HNSWLIB = True\n",
    "except ImportError:\n",
    "    _HAVE_HNSWLIB = False\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    import tensorflow_hub as hub\n",
    "    import tensorflow_text\n",
    "\n",
    "    _HAVE_TENSORFLOW = True\n",
    "except ImportError:\n",
    "    _HAVE_TENSORFLOW = False\n",
    "\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "\n",
    "    _HAVE_TORCH = True\n",
    "except ImportError:\n",
    "    _HAVE_TORCH = False\n",
    "\n",
    "logger = logging.getLogger('top2vec')\n",
    "logger.setLevel(logging.WARNING)\n",
    "sh = logging.StreamHandler()\n",
    "sh.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))\n",
    "logger.addHandler(sh)\n",
    "\n",
    "\n",
    "def default_tokenizer(doc):\n",
    "    \"\"\"Tokenize documents for training and remove too long/short words\"\"\"\n",
    "    return simple_preprocess(strip_tags(doc), deacc=True)\n",
    "\n",
    "\n",
    "class Top2VecNew(Top2Vec):\n",
    "    \"\"\"\n",
    "    Top2Vec\n",
    "    Creates jointly embedded topic, document and word vectors.\n",
    "    Parameters\n",
    "    ----------\n",
    "    embedding_model: string\n",
    "        This will determine which model is used to generate the document and\n",
    "        word embeddings. The valid string options are:\n",
    "            * doc2vec\n",
    "            * universal-sentence-encoder\n",
    "            * universal-sentence-encoder-multilingual\n",
    "            * distiluse-base-multilingual-cased\n",
    "        For large data sets and data sets with very unique vocabulary doc2vec\n",
    "        could produce better results. This will train a doc2vec model from\n",
    "        scratch. This method is language agnostic. However multiple languages\n",
    "        will not be aligned.\n",
    "        Using the universal sentence encoder options will be much faster since\n",
    "        those are pre-trained and efficient models. The universal sentence\n",
    "        encoder options are suggested for smaller data sets. They are also\n",
    "        good options for large data sets that are in English or in languages\n",
    "        covered by the multilingual model. It is also suggested for data sets\n",
    "        that are multilingual.\n",
    "        For more information on universal-sentence-encoder visit:\n",
    "        https://tfhub.dev/google/universal-sentence-encoder/4\n",
    "        For more information on universal-sentence-encoder-multilingual visit:\n",
    "        https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\n",
    "        The distiluse-base-multilingual-cased pre-trained sentence transformer\n",
    "        is suggested for multilingual datasets and languages that are not\n",
    "        covered by the multilingual universal sentence encoder. The\n",
    "        transformer is significantly slower than the universal sentence\n",
    "        encoder options.\n",
    "        For more informati ond istiluse-base-multilingual-cased visit:\n",
    "        https://www.sbert.net/docs/pretrained_models.html\n",
    "    embedding_model_path: string (Optional)\n",
    "        Pre-trained embedding models will be downloaded automatically by\n",
    "        default. However they can also be uploaded from a file that is in the\n",
    "        location of embedding_model_path.\n",
    "        Warning: the model at embedding_model_path must match the\n",
    "        embedding_model parameter type.\n",
    "    documents: List of str\n",
    "        Input corpus, should be a list of strings.\n",
    "    min_count: int (Optional, default 50)\n",
    "        Ignores all words with total frequency lower than this. For smaller\n",
    "        corpora a smaller min_count will be necessary.\n",
    "    speed: string (Optional, default 'learn')\n",
    "        This parameter is only used when using doc2vec as embedding_model.\n",
    "        It will determine how fast the model takes to train. The\n",
    "        fast-learn option is the fastest and will generate the lowest quality\n",
    "        vectors. The learn option will learn better quality vectors but take\n",
    "        a longer time to train. The deep-learn option will learn the best\n",
    "        quality vectors but will take significant time to train. The valid\n",
    "        string speed options are:\n",
    "        \n",
    "            * fast-learn\n",
    "            * learn\n",
    "            * deep-learn\n",
    "    use_corpus_file: bool (Optional, default False)\n",
    "        This parameter is only used when using doc2vec as embedding_model.\n",
    "        Setting use_corpus_file to True can sometimes provide speedup for\n",
    "        large datasets when multiple worker threads are available. Documents\n",
    "        are still passed to the model as a list of str, the model will create\n",
    "        a temporary corpus file for training.\n",
    "    document_ids: List of str, int (Optional)\n",
    "        A unique value per document that will be used for referring to\n",
    "        documents in search results. If ids are not given to the model, the\n",
    "        index of each document in the original corpus will become the id.\n",
    "    keep_documents: bool (Optional, default True)\n",
    "        If set to False documents will only be used for training and not saved\n",
    "        as part of the model. This will reduce model size. When using search\n",
    "        functions only document ids will be returned, not the actual\n",
    "        documents.\n",
    "    workers: int (Optional)\n",
    "        The amount of worker threads to be used in training the model. Larger\n",
    "        amount will lead to faster training.\n",
    "    \n",
    "    tokenizer: callable (Optional, default None)\n",
    "        Override the default tokenization method. If None then\n",
    "        gensim.utils.simple_preprocess will be used.\n",
    "    use_embedding_model_tokenizer: bool (Optional, default False)\n",
    "        If using an embedding model other than doc2vec, use the model's\n",
    "        tokenizer for document embedding. If set to True the tokenizer, either\n",
    "        default or passed callable will be used to tokenize the text to\n",
    "        extract the vocabulary for word embedding.\n",
    "    umap_args: dict (Optional, default None)\n",
    "        Pass custom arguments to UMAP.\n",
    "    hdbscan_args: dict (Optional, default None)\n",
    "        Pass custom arguments to HDBSCAN.\n",
    "    \n",
    "    verbose: bool (Optional, default True)\n",
    "        Whether to print status data during training.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 documents,\n",
    "                 min_count=50,\n",
    "                 embedding_model='doc2vec',\n",
    "                 embedding_model_path=None,\n",
    "                 speed='learn',\n",
    "                 use_corpus_file=False,\n",
    "                 document_ids=None,\n",
    "                 keep_documents=True,\n",
    "                 workers=None,\n",
    "                 tokenizer=None,\n",
    "                 use_embedding_model_tokenizer=False,\n",
    "                 umap_args=None,\n",
    "                 hdbscan_args=None,\n",
    "                 verbose=True\n",
    "                 ):\n",
    "\n",
    "        if verbose:\n",
    "            logger.setLevel(logging.DEBUG)\n",
    "            self.verbose = True\n",
    "        else:\n",
    "            logger.setLevel(logging.WARNING)\n",
    "            self.verbose = False\n",
    "\n",
    "        if tokenizer is None:\n",
    "            tokenizer = default_tokenizer\n",
    "\n",
    "        # validate documents\n",
    "        if not (isinstance(documents, list) or isinstance(documents, np.ndarray)):\n",
    "            raise ValueError(\"Documents need to be a list of strings\")\n",
    "        if not all((isinstance(doc, str) or isinstance(doc, np.str_)) for doc in documents):\n",
    "            raise ValueError(\"Documents need to be a list of strings\")\n",
    "        if keep_documents:\n",
    "            self.documents = np.array(documents, dtype=\"object\")\n",
    "        else:\n",
    "            self.documents = None\n",
    "\n",
    "        # validate document ids\n",
    "        if document_ids is not None:\n",
    "            if not (isinstance(document_ids, list) or isinstance(document_ids, np.ndarray)):\n",
    "                raise ValueError(\"Documents ids need to be a list of str or int\")\n",
    "\n",
    "            if len(documents) != len(document_ids):\n",
    "                raise ValueError(\"Document ids need to match number of documents\")\n",
    "            elif len(document_ids) != len(set(document_ids)):\n",
    "                raise ValueError(\"Document ids need to be unique\")\n",
    "\n",
    "            if all((isinstance(doc_id, str) or isinstance(doc_id, np.str_)) for doc_id in document_ids):\n",
    "                self.doc_id_type = np.str_\n",
    "            elif all((isinstance(doc_id, int) or isinstance(doc_id, np.int_)) for doc_id in document_ids):\n",
    "                self.doc_id_type = np.int_\n",
    "            else:\n",
    "                raise ValueError(\"Document ids need to be str or int\")\n",
    "\n",
    "            self.document_ids_provided = True\n",
    "            self.document_ids = np.array(document_ids)\n",
    "            self.doc_id2index = dict(zip(document_ids, list(range(0, len(document_ids)))))\n",
    "        else:\n",
    "            self.document_ids_provided = False\n",
    "            self.document_ids = np.array(range(0, len(documents)))\n",
    "            self.doc_id2index = dict(zip(self.document_ids, list(range(0, len(self.document_ids)))))\n",
    "            self.doc_id_type = np.int_\n",
    "\n",
    "        acceptable_embedding_models = [\"universal-sentence-encoder-multilingual\",\n",
    "                                       \"universal-sentence-encoder\",\n",
    "                                       \"all-mpnet-base-v2\"]\n",
    "\n",
    "        self.embedding_model_path = embedding_model_path\n",
    "\n",
    "        if embedding_model == 'doc2vec':\n",
    "\n",
    "            # validate training inputs\n",
    "            if speed == \"fast-learn\":\n",
    "                hs = 0\n",
    "                negative = 5\n",
    "                epochs = 40\n",
    "            elif speed == \"learn\":\n",
    "                hs = 1\n",
    "                negative = 0\n",
    "                epochs = 40\n",
    "            elif speed == \"deep-learn\":\n",
    "                hs = 1\n",
    "                negative = 0\n",
    "                epochs = 400\n",
    "            elif speed == \"test-learn\":\n",
    "                hs = 0\n",
    "                negative = 5\n",
    "                epochs = 1\n",
    "            else:\n",
    "                raise ValueError(\"speed parameter needs to be one of: fast-learn, learn or deep-learn\")\n",
    "\n",
    "            if workers is None:\n",
    "                pass\n",
    "            elif isinstance(workers, int):\n",
    "                pass\n",
    "            else:\n",
    "                raise ValueError(\"workers needs to be an int\")\n",
    "\n",
    "            doc2vec_args = {\"vector_size\": 300,\n",
    "                            \"min_count\": min_count,\n",
    "                            \"window\": 15,\n",
    "                            \"sample\": 1e-5,\n",
    "                            \"negative\": negative,\n",
    "                            \"hs\": hs,\n",
    "                            \"epochs\": epochs,\n",
    "                            \"dm\": 0,\n",
    "                            \"dbow_words\": 1}\n",
    "\n",
    "            if workers is not None:\n",
    "                doc2vec_args[\"workers\"] = workers\n",
    "\n",
    "            logger.info('Pre-processing documents for training')\n",
    "\n",
    "            if use_corpus_file:\n",
    "                processed = [' '.join(tokenizer(doc)) for doc in documents]\n",
    "                lines = \"\\n\".join(processed)\n",
    "                temp = tempfile.NamedTemporaryFile(mode='w+t')\n",
    "                temp.write(lines)\n",
    "                doc2vec_args[\"corpus_file\"] = temp.name\n",
    "\n",
    "\n",
    "            else:\n",
    "                train_corpus = [TaggedDocument(tokenizer(doc), [i]) for i, doc in enumerate(documents)]\n",
    "                doc2vec_args[\"documents\"] = train_corpus\n",
    "\n",
    "            logger.info('Creating joint document/word embedding')\n",
    "            self.embedding_model = 'doc2vec'\n",
    "            self.model = Doc2Vec(**doc2vec_args)\n",
    "\n",
    "            if use_corpus_file:\n",
    "                temp.close()\n",
    "\n",
    "        elif embedding_model in acceptable_embedding_models:\n",
    "\n",
    "            self.embed = None\n",
    "            self.embedding_model = embedding_model\n",
    "\n",
    "            self._check_import_status()\n",
    "\n",
    "            logger.info('Pre-processing documents for training')\n",
    "\n",
    "            # preprocess documents\n",
    "            tokenized_corpus = [tokenizer(doc) for doc in documents]\n",
    "\n",
    "            def return_doc(doc):\n",
    "                return doc\n",
    "\n",
    "            # preprocess vocabulary\n",
    "            vectorizer = CountVectorizer(tokenizer=return_doc, preprocessor=return_doc)\n",
    "            doc_word_counts = vectorizer.fit_transform(tokenized_corpus)\n",
    "            words = vectorizer.get_feature_names()\n",
    "            word_counts = np.array(np.sum(doc_word_counts, axis=0).tolist()[0])\n",
    "            vocab_inds = np.where(word_counts > min_count)[0]\n",
    "\n",
    "            if len(vocab_inds) == 0:\n",
    "                raise ValueError(f\"A min_count of {min_count} results in \"\n",
    "                                 f\"all words being ignored, choose a lower value.\")\n",
    "            self.vocab = [words[ind] for ind in vocab_inds]\n",
    "\n",
    "            self._check_model_status()\n",
    "\n",
    "            logger.info('Creating joint document/word embedding')\n",
    "\n",
    "            # embed words\n",
    "            self.word_indexes = dict(zip(self.vocab, range(len(self.vocab))))\n",
    "            self.word_vectors = self._l2_normalize(np.array(self.embed(self.vocab)))\n",
    "\n",
    "            # embed documents\n",
    "            if use_embedding_model_tokenizer:\n",
    "                self.document_vectors = self._embed_documents(documents)\n",
    "            else:\n",
    "                train_corpus = [' '.join(tokens) for tokens in tokenized_corpus]\n",
    "                self.document_vectors = self._embed_documents(train_corpus)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"{embedding_model} is an invalid embedding model.\")\n",
    "\n",
    "        # create 5D embeddings of documents\n",
    "        logger.info('Creating lower dimension embedding of documents')\n",
    "\n",
    "        if umap_args is None:\n",
    "            umap_args = {'n_neighbors': 15,\n",
    "                         'n_components': 5,\n",
    "                         'metric': 'cosine'}\n",
    "\n",
    "        umap_model = umap.UMAP(**umap_args).fit(self._get_document_vectors(norm=False))\n",
    "\n",
    "        # find dense areas of document vectors\n",
    "        logger.info('Finding dense areas of documents')\n",
    "\n",
    "        if hdbscan_args is None:\n",
    "            hdbscan_args = {'min_cluster_size': 15,\n",
    "                            'metric': 'euclidean',\n",
    "                            'cluster_selection_method': 'eom'}\n",
    "\n",
    "        cluster = hdbscan.HDBSCAN(**hdbscan_args).fit(umap_model.embedding_)\n",
    "\n",
    "        # calculate topic vectors from dense areas of documents\n",
    "        logger.info('Finding topics')\n",
    "\n",
    "        # create topic vectors\n",
    "        self._create_topic_vectors(cluster.labels_)\n",
    "\n",
    "        # deduplicate topics\n",
    "        self._deduplicate_topics()\n",
    "\n",
    "        # find topic words and scores\n",
    "        self.topic_words, self.topic_word_scores = self._find_topic_words_and_scores(topic_vectors=self.topic_vectors)\n",
    "\n",
    "        # assign documents to topic\n",
    "        self.doc_top, self.doc_dist = self._calculate_documents_topic(self.topic_vectors,\n",
    "                                                                      self._get_document_vectors())\n",
    "\n",
    "        # calculate topic sizes\n",
    "        self.topic_sizes = self._calculate_topic_sizes(hierarchy=False)\n",
    "\n",
    "        # re-order topics\n",
    "        self._reorder_topics(hierarchy=False)\n",
    "\n",
    "        # initialize variables for hierarchical topic reduction\n",
    "        self.topic_vectors_reduced = None\n",
    "        self.doc_top_reduced = None\n",
    "        self.doc_dist_reduced = None\n",
    "        self.topic_sizes_reduced = None\n",
    "        self.topic_words_reduced = None\n",
    "        self.topic_word_scores_reduced = None\n",
    "        self.hierarchy = None\n",
    "\n",
    "        # initialize document indexing variables\n",
    "        self.document_index = None\n",
    "        self.serialized_document_index = None\n",
    "        self.documents_indexed = False\n",
    "        self.index_id2doc_id = None\n",
    "        self.doc_id2index_id = None\n",
    "\n",
    "        # initialize word indexing variables\n",
    "        self.word_index = None\n",
    "        self.serialized_word_index = None\n",
    "        self.words_indexed = False\n",
    "\n",
    "    def _check_import_status(self):\n",
    "        if self.embedding_model != 'all-mpnet-base-v2':\n",
    "            if not _HAVE_TENSORFLOW:\n",
    "                raise ImportError(f\"{self.embedding_model} is not available.\\n\\n\"\n",
    "                                  \"Try: pip install top2vec[sentence_encoders]\\n\\n\"\n",
    "                                  \"Alternatively try: pip install tensorflow tensorflow_hub tensorflow_text\")\n",
    "        else:\n",
    "            if not _HAVE_TORCH:\n",
    "                raise ImportError(f\"{self.embedding_model} is not available.\\n\\n\"\n",
    "                                  \"Try: pip install top2vec[sentence_transformers]\\n\\n\"\n",
    "                                  \"Alternatively try: pip install torch sentence_transformers\")\n",
    "\n",
    "    def _check_model_status(self):\n",
    "        if self.embed is None:\n",
    "            if self.verbose is False:\n",
    "                logger.setLevel(logging.DEBUG)\n",
    "\n",
    "            if self.embedding_model != \"all-mpnet-base-v2\":\n",
    "                if self.embedding_model_path is None:\n",
    "                    logger.info(f'Downloading {self.embedding_model} model')\n",
    "                    if self.embedding_model == \"universal-sentence-encoder-multilingual\":\n",
    "                        module = \"https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\"\n",
    "                    else:\n",
    "                        module = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    "                else:\n",
    "                    logger.info(f'Loading {self.embedding_model} model at {self.embedding_model_path}')\n",
    "                    module = self.embedding_model_path\n",
    "                self.embed = hub.load(module)\n",
    "\n",
    "            else:\n",
    "                if self.embedding_model_path is None:\n",
    "                    logger.info(f'Downloading {self.embedding_model} model')\n",
    "                    module = 'all-mpnet-base-v2'\n",
    "                else:\n",
    "                    logger.info(f'Loading {self.embedding_model} model at {self.embedding_model_path}')\n",
    "                    module = self.embedding_model_path\n",
    "                model = SentenceTransformer(module)\n",
    "                self.embed = model.encode\n",
    "\n",
    "        if self.verbose is False:\n",
    "            logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oz16gmSjHcs6"
   },
   "source": [
    "We can then use this `Top2VecNew` class to run our experiments including the `\"all-mpnet-base-v2\"` model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NdPp3Xsda_SJ"
   },
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    dataset, custom = \"trump\", True\n",
    "    params = {\"nr_topics\": [(i+1)*10 for i in range(5)],\n",
    "              # \"embedding_model\": \"all-mpnet-base-v2\",\n",
    "              \"hdbscan_args\": {'min_cluster_size': 15,\n",
    "                               'metric': 'euclidean',\n",
    "                               'cluster_selection_method': 'eom'}}\n",
    "\n",
    "    trainer = Trainer(dataset=dataset,\n",
    "                      custom_dataset=custom,\n",
    "                      custom_model=Top2VecNew,\n",
    "                      model_name=\"Top2Vec\",\n",
    "                      params=params,\n",
    "                      verbose=True)\n",
    "    results = trainer.train(save=f\"Top2Vec_trump_{i+1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WKxdYyJECpuY"
   },
   "source": [
    "# **DTM Evaluation**\n",
    "\n",
    "Here, we evaluate BERTopic and LDAseq on a dynamic topic modeling task with two datasets: \n",
    "* Trump's tweets\n",
    "* UN general debates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "67jrSUR6DPbi"
   },
   "source": [
    "### **BERTopic**\n",
    "\n",
    "As seen before, we can load our data and generate embeddings before passing it to our evaluator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vl4xsRDgCtSY"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Prepare data\n",
    "dataset, custom = \"trump_dtm\", True\n",
    "data_loader = DataLoader(dataset)\n",
    "_, timestamps = data_loader.load_docs()\n",
    "data = data_loader.load_octis(custom)\n",
    "data = [\" \".join(words) for words in data.get_corpus()]\n",
    "\n",
    "# Extract embeddings\n",
    "model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "embeddings = model.encode(data, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "snszIP5bIbE4"
   },
   "source": [
    "Then, we also need to make sure that the timestamps match the data that were are using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7NhqOi6hqJDJ"
   },
   "outputs": [],
   "source": [
    "# Match indices\n",
    "import os\n",
    "os.listdir(f\"./{dataset}\")\n",
    "with open(f\"./{dataset}/indexes.txt\") as f:\n",
    "    indices = f.readlines()\n",
    "    \n",
    "indices = [int(index.split(\"\\n\")[0]) for index in indices]\n",
    "timestamps = [timestamp for index, timestamp in enumerate(timestamps) if index in indices]\n",
    "len(data), len(timestamps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q7Luc2euIfVL"
   },
   "source": [
    "Finally, we can simply run the Trainer as we did before but adding the timestamps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uqSmtipeIm8n"
   },
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    params = {\n",
    "        \"nr_topics\": [50],\n",
    "        \"min_topic_size\": 15,\n",
    "        \"verbose\": True,\n",
    "    }\n",
    "\n",
    "    trainer = Trainer(dataset=dataset,\n",
    "                      model_name=\"BERTopic\",\n",
    "                      params=params,\n",
    "                      bt_embeddings=embeddings,\n",
    "                      custom_dataset=custom,\n",
    "                      bt_timestamps=timestamps,\n",
    "                      topk=5,\n",
    "                      bt_nr_bins=10,\n",
    "                      verbose=True)\n",
    "    results = trainer.train(f\"DynamicBERTopic_trump_{i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZF4AFG2oDQn-"
   },
   "source": [
    "### **LDAseq**\n",
    "To run LDAseq, we again prepare our data and match the indices of our timestamps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hQ4X6GymFYSu",
    "outputId": "22ab5c57-a279-48de-dc91-966c33a885c7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "119320it [03:25, 579.62it/s]\n",
      "278837it [00:00, 1751620.37it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(273743, 273743)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Prepare data\n",
    "dataset, custom = \"un_dtm\", True\n",
    "data_loader = DataLoader(dataset)\n",
    "_, timestamps = data_loader.load_docs()\n",
    "data = data_loader.load_octis(custom)\n",
    "data = [\" \".join(words) for words in data.get_corpus()]\n",
    "\n",
    "# Match indices\n",
    "os.listdir(f\"{dataset}\")\n",
    "with open(f\"{dataset}/indexes.txt\") as f:\n",
    "    indices = f.readlines()\n",
    "    \n",
    "indices = [int(index.split(\"\\n\")[0]) for index in indices]\n",
    "indices_test = {index: True for index in indices}\n",
    "timestamps = [timestamp for index, timestamp in tqdm(enumerate(timestamps)) if indices_test.get(index)]\n",
    "len(data), len(timestamps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VAonxNdtJYwD"
   },
   "source": [
    "Then, we simply pass the timestamps and run our the trainer for LDAseq:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4wv_5-awJ4g7"
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"num_topics\": [50],\n",
    "    \"nr_bins\": 9,\n",
    "    \"random_state\": 42\n",
    "}\n",
    "\n",
    "trainer = Trainer(dataset=dataset,\n",
    "                  model_name=\"LDAseq\",\n",
    "                  params=params,\n",
    "                  custom_dataset=custom,\n",
    "                  bt_timestamps=timestamps,\n",
    "                  topk=5,\n",
    "                  verbose=True)\n",
    "results = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uNKfK8C3KH1M"
   },
   "source": [
    "We remove some information from the results as those are quite big to save:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6vB77Xa2qYPZ"
   },
   "outputs": [],
   "source": [
    "results[0][\"Params\"].keys()\n",
    "del results[0][\"Params\"][\"corpus\"]\n",
    "del results[0][\"Params\"][\"id2word\"]\n",
    "del results[0][\"Params\"][\"time_slice\"]\n",
    "\n",
    "import json\n",
    "with open(f\"LDAseq_trump.json\", 'w') as f:\n",
    "    json.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j-M13ABjKQYE"
   },
   "source": [
    "# **Wall time**\n",
    "Here, we only focus on the wall time of each topic model, from instantiating the model to training. To do so, we take the Trump dataset and split it up into steps of 1000 documents. Then, we can train a model and track the wall time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7l0Yu9vSKfKc"
   },
   "outputs": [],
   "source": [
    "embedding_model = \"all-mpnet-base-v2\"\n",
    "# embedding_model = tensorflow_hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "embedding_model_name = \"all-mpnet-base-v2\"\n",
    "topic_model_name = \"BERTopic_USE\"\n",
    "\n",
    "results = pd.DataFrame(columns=[\"dataset\", \"nr_documents\", \"vocab_size\", \"time\",\n",
    "                                \"cpu\", \"gpu\", \"gpu_cudnn\", \"gpu_memory\", \"embedding_model\"])\n",
    "for index, nr_documents in enumerate(tqdm(np.arange(1000, len(data), 2_000, dtype=int))):\n",
    "    \n",
    "    selected_data = random.sample(data, nr_documents)\n",
    "    selected_tokenized_data = random.sample(tokenized_data, nr_documents)\n",
    "    \n",
    "    if topic_model_name == \"CTM\":\n",
    "        qt, training_dataset = preprocess_ctm(selected_data, embedding_model_name)\n",
    "    \n",
    "    # Run model\n",
    "    start = time.time()\n",
    "    \n",
    "    if topic_model_name == \"LDA\":\n",
    "        id2word = corpora.Dictionary(selected_tokenized_data)\n",
    "        id_corpus = [id2word.doc2bow(document) for document in selected_tokenized_data]\n",
    "        lda = LdaMulticore(id_corpus, id2word=id2word, num_topics=100)\n",
    "    \n",
    "    elif topic_model_name == \"NFM\":\n",
    "        id2word = corpora.Dictionary(selected_tokenized_data)\n",
    "        id_corpus = [id2word.doc2bow(document) for document in selected_tokenized_data]\n",
    "        nmf_model = nmf.Nmf(id_corpus, id2word=id2word, num_topics=100)\n",
    "\n",
    "    elif topic_model_name == \"BERTopic\":\n",
    "        topic_model = BERTopic(embedding_model=embedding_model)    \n",
    "        topics, probs = topic_model.fit_transform(selected_data)\n",
    "        \n",
    "    elif topic_model_name == \"BERTopic_Doc2Vec\":\n",
    "        train_corpus = [TaggedDocument(default_tokenizer(doc), [i]) for i, doc in enumerate(selected_data)]\n",
    "        doc2vec_args = {\"vector_size\": 300,\n",
    "                        \"min_count\": 50,\n",
    "                        \"window\": 15,\n",
    "                        \"sample\": 1e-5,\n",
    "                        \"negative\": 0,\n",
    "                        \"hs\": 1,\n",
    "                        \"epochs\": 40,\n",
    "                        \"dm\": 0,\n",
    "                        \"dbow_words\": 1,\n",
    "                       \"documents\": train_corpus,\n",
    "                       \"workers\": -1}\n",
    "        model = Doc2Vec(**doc2vec_args)\n",
    "        embeddings = model.docvecs.vectors_docs\n",
    "        topic_model = BERTopic()    \n",
    "        topics, probs = topic_model.fit_transform(selected_data, embeddings)\n",
    "        \n",
    "    elif topic_model_name == \"BERTopic_USE\":\n",
    "        embeddings = embedding_model(selected_data).cpu().numpy()\n",
    "        topic_model = BERTopic(embedding_model=embedding_model)    \n",
    "        topics, probs = topic_model.fit_transform(selected_data, embeddings)\n",
    "\n",
    "    elif topic_model_name == \"Top2Vec\":\n",
    "        model = Top2Vec(selected_data, hdbscan_args={\"min_cluster_size\": 15}, workers=-1)\n",
    "#         model = Top2VecNew(selected_data, hdbscan_args={\"min_cluster_size\": 15}, embedding_model=embedding_model)\n",
    "        \n",
    "    elif topic_model_name == \"CTM\":\n",
    "        ctm = CombinedTM(n_components=100, contextual_size=768, bow_size=len(qt.vocab))\n",
    "        ctm.fit(training_dataset)\n",
    "    \n",
    "    end = time.time()\n",
    "\n",
    "    # Calculate vocab size\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(selected_data)\n",
    "    vocab_size = len(vectorizer.get_feature_names())\n",
    "    \n",
    "    results.loc[len(results)] = [dataset, len(selected_data), vocab_size, end - start, cpu_name, gpu_name, \n",
    "                                 gpu_cudnn, gpu_memory, embedding_model_name]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BERTopic OCTIS v3.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
